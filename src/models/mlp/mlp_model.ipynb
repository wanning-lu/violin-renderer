{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1707100535892,
     "user": {
      "displayName": "Wanning Lu",
      "userId": "09623306388777366715"
     },
     "user_tz": 480
    },
    "id": "ot-KHBHSzKbw"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Run data_processing and batching_data\n",
    "%run ~/violin-renderer/src/models/mlp/data_processing.ipynb\n",
    "\n",
    "# Initialize GPU to move model/tensors onto\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "EPOCHS = 100\n",
    "\n",
    "# Number of nodes in hidden layer\n",
    "HIDDEN_LAYER_SIZE = 8\n",
    "\n",
    "# Early Stopping\n",
    "TOLERANCE = 5\n",
    "\n",
    "# store train and validation loss\n",
    "TRAIN_STEP_LOSS = {} # (epoch, step) : loss\n",
    "TRAIN_EPOCH_LOSS = {} # epoch : average loss\n",
    "VALIDATE_LOSS = {} # epoch : average loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1707100535893,
     "user": {
      "displayName": "Wanning Lu",
      "userId": "09623306388777366715"
     },
     "user_tz": 480
    },
    "id": "3FWYp4JezKb3"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.BatchNorm1d(hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self):\n",
    "        self.min_validation_loss = float(\"inf\")\n",
    "        self.tolerance = TOLERANCE\n",
    "        self.current_strike = 0\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, validation_loss):\n",
    "        if validation_loss > self.min_validation_loss:\n",
    "            self.current_strike += 1\n",
    "            # if we get #(tolerance) in a row\n",
    "            if self.current_strike >= self.tolerance:  \n",
    "                self.early_stop = True\n",
    "        else: \n",
    "            # reset strike\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.current_strike = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1707100536110,
     "user": {
      "displayName": "Wanning Lu",
      "userId": "09623306388777366715"
     },
     "user_tz": 480
    },
    "id": "NAy7tNjezKb5"
   },
   "outputs": [],
   "source": [
    "# Trains the model inputted into the function.\n",
    "\n",
    "# @param model: The model object to be trained\n",
    "# @param optimizer: The optimizing equation to use to train the model\n",
    "# @param data_loader: Values for source_input_note and ground_truth\n",
    "# @param loss_module: Equation for calculating the difference between generated and actual output\n",
    "# @param epoch: Current epoch iteration\n",
    "# @return: avearge loss for the loop\n",
    "def train_model_loop(model, optimizer, data_loader, loss_module, epoch):\n",
    "    # Set model to train mode\n",
    "    model.train()\n",
    "\n",
    "    # Training loop \n",
    "    total_loss = 0  \n",
    "    for batch, (source_input_note, ground_truth) in enumerate(data_loader):\n",
    "        # Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "        source_input_note = source_input_note.to(device)\n",
    "        ground_truth = ground_truth.to(device)\n",
    "\n",
    "        # Step 2: Run the model on the input data\n",
    "        predictions = model(source_input_note)\n",
    "\n",
    "        # Step 3: Calculate the loss\n",
    "        loss = loss_module(predictions, ground_truth)\n",
    "\n",
    "        # Step 4: Perform backpropagation\n",
    "        # Before calculating the gradients, we need to ensure that they are all zero.\n",
    "        # The gradients would not be overwritten, but actually added to the existing ones.\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Step 5: Update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # Step 6: Get loss values to store and print\n",
    "        loss = loss.item()\n",
    "        total_loss += loss\n",
    "\n",
    "        # Step 7: For every 50th batch, print out the current loss as well # of samples trained\n",
    "        if batch % 50 == 0:\n",
    "            samples_trained = batch * 100 + len(source_input_note)\n",
    "            print(f\"Loss: {loss:>7f} [{samples_trained}/{len(data_loader.dataset)}]\")\n",
    "\n",
    "        # Step 8: store in TRAIN_STEP_LOSS\n",
    "        TRAIN_STEP_LOSS[(epoch, batch)] = loss\n",
    "    \n",
    "    return total_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run on validation dataset\n",
    "\n",
    "# @param model: Current MLP model\n",
    "# @param data_loader: Values for source_input_note and ground_truth\n",
    "# @param loss_module: Equation for calculating the difference between generated and actual output\n",
    "def run_on_validation_dataset(model, data_loader, loss_module):\n",
    "    loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch, (source_input_note, ground_truth) in enumerate(data_loader):\n",
    "            ## Step 1: Move input data to device (only strictly necessary if we use GPU)\n",
    "            source_input_note = source_input_note.to(device)\n",
    "            ground_truth = ground_truth.to(device)\n",
    "\n",
    "            ## Step 2: Run the model on the input data\n",
    "            predictions = model(source_input_note)\n",
    "\n",
    "            ## Step 3: Calculate the loss\n",
    "            loss += loss_module(predictions, ground_truth)\n",
    "    \n",
    "    # return the average loss value\n",
    "    return loss.item() / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains the model using the data_loader\n",
    "\n",
    "# @param model: The model object to be trained\n",
    "# @param optimizer: The optimizing equation to use to train the model\n",
    "# @param training_loader: Values for source_input_note and ground_truth for the training dataset\n",
    "# @param validating_loader: Values for source_input_note and ground_truth for the validating dataset\n",
    "# @param loss_module: Equation for calculating the difference between generated and actual output\n",
    "# @param name: Name of model want to save as\n",
    "def train_model(model, optimizer, training_loader, validating_loader, loss_module, name):\n",
    "    early_stopping = EarlyStopping()\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        \n",
    "        # Train 1 time and store in TRAIN_STEP_LOSS\n",
    "        training_loss_value = train_model_loop(model, optimizer, training_loader, loss_module, epoch)\n",
    "        TRAIN_EPOCH_LOSS[epoch] = training_loss_value\n",
    "\n",
    "        # Get loss value using validation test and store in VALIDATE_LOSS\n",
    "        validating_loss_value = run_on_validation_dataset(model, validating_loader, loss_module)\n",
    "        VALIDATE_LOSS[epoch] = validating_loss_value\n",
    "\n",
    "        # Check for early stopping\n",
    "        early_stopping(validating_loss_value)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"We stop at epoch:\", epoch)\n",
    "            break\n",
    "\n",
    "    torch.save(model.state_dict(), HOME_PATH + '/src/models/mlp/states/' + name + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates error between generated output and testing truth\n",
    "\n",
    "# @param mlp_model: mlp model to use\n",
    "# @return: an average MSE for all songs in the testing dataset\n",
    "def MSE_error(mlp_model):\n",
    "    testing_results = generate_all_testing_data(mlp_model)\n",
    "    _ , testing_ground_truths = processed_testing_datasets()\n",
    "    \n",
    "    MSE_results = []\n",
    "    for generative_timings, ground_truth in zip(testing_results.values(), testing_ground_truths.values()):\n",
    "        predict_values = torch.Tensor(generative_timings)\n",
    "        ground_truth_values = torch.Tensor(ground_truth)\n",
    "        MSE_value = loss(predict_values, ground_truth_values)\n",
    "        MSE_results.append(MSE_value)\n",
    "\n",
    "    return MSE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1707100552271,
     "user": {
      "displayName": "Wanning Lu",
      "userId": "09623306388777366715"
     },
     "user_tz": 480
    },
    "id": "U_auXk9GzKb6",
    "outputId": "3fac618e-1428-49c7-c066-4f4e817df80d"
   },
   "outputs": [],
   "source": [
    "# this creates a dictionary of outputs for all pieces in the testing dataset\n",
    "\n",
    "# @param mlp_model: mlp model to use\n",
    "# @returns a dictionary mapping with key of file path and value of start and end\n",
    "def generate_all_testing_data(mlp_model):\n",
    "    mlp_model.to(device)\n",
    "    testing_source_inputs, _ = processed_testing_datasets()\n",
    "\n",
    "    # generating an output for each piece in the testing input dataset\n",
    "    testing_results = {}\n",
    "    for song_path, song_notes in testing_source_inputs.items():\n",
    "        test_input = torch.Tensor(song_notes)\n",
    "        test_input = test_input.to(device)\n",
    "\n",
    "        # generate start and end timings\n",
    "        predictions = mlp_model(test_input)\n",
    "\n",
    "        # append to map\n",
    "        predictions = predictions.tolist()\n",
    "        testing_results[song_path] = predictions\n",
    "\n",
    "    return testing_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get processed training and validating datasets\n",
    "training_source_inputs, training_ground_truths = processed_training_datasets()\n",
    "validating_source_inputs, validating_ground_truths = processed_validating_datasets()\n",
    "\n",
    "# Initializing MLPMusicDataset objects\n",
    "training_MLPMusicDataset = MLPMusicDataset(source_input_data=torch.Tensor(training_source_inputs), ground_truth_data=torch.Tensor(training_ground_truths))\n",
    "validating_MLPMusicDataset = MLPMusicDataset(source_input_data=torch.Tensor(validating_source_inputs), ground_truth_data=torch.Tensor(validating_ground_truths))\n",
    "\n",
    "# Load data to create batches\n",
    "training_loader = DataLoader(training_MLPMusicDataset, batch_size=100, shuffle=True)\n",
    "validating_loader = DataLoader(validating_MLPMusicDataset, batch_size=100, shuffle=False)\n",
    "\n",
    "# initialize the MLP model\n",
    "mlp_model = MLP(input_size=3, hidden_size=HIDDEN_LAYER_SIZE, output_size=2)\n",
    "\n",
    "# transfer model to GPU\n",
    "mlp_model.to(device)\n",
    "\n",
    "# Define our loss function (mean squared error) to be used in the grad descent step\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "# Performs the gradient descent steps\n",
    "optimizer = torch.optim.Adam(mlp_model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "# train_model(mlp_model, optimizer, training_loader, validating_loader, loss, name=\"mlp_model\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
